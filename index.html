<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Digital humanities research project exploring [your topic]">
    <meta name="author" content="Your Name">
    <title>IT Reviews | WRIT 20833</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#question">Research Question</a></li>
            <li><a href="#data">Data & Methods</a></li>
            <li><a href="#analysis">Results & Analysis</a></li>
            <li><a href="#findings">Findings</a></li>
            <li><a href="#reflection">Reflection</a></li>
        </ul>
    </nav>

    <header>
        <h1>IT Reviews</h1>
        <p>Lucy Fulghum and Reagan Nowak | WRIT 20833 | Fall 2025</p>
    </header>

    <main>
        <section id="question">
            <h2>Research Question</h2>
            <!-- TODO: Add your research question here -->
            <p>How do reviewers of IT use emotional language to process trauma? What does that reveal about horror as a cultural space for collective healing?</p>

            <p><strong>Background:</strong> This question matters because horror films occupy a unique cultural space where audiences confront fear in a controlled environment. It particularily deals with childhood trauma, bullying, and the lingering effects of fear, making it a meaningful text for exploring how people process emotionally charged experiences. Online reviews offer an unfiltered window into how viewers articulate those reactions. The observation that many reviews seemed to use emotional or trauma-related language  to reflect on personal experiences, drove this research. By analyzing these reviews computationally, we can better understand how horror films not only funtion as entertainment but also as a space for collective healing, emotional expression, and the negotiation of trauma.</p>
        </section>

        <section id="data">
            <h2>Data & Methods</h2>

            <h3>Dataset</h3>
            <!-- TODO: Describe your data collection and methodology -->
            <p><strong>Data Source:</strong> We collected data from the reviews of IT on the site IMDB.</p>
            <p><strong>Collection Method:</strong> We used instant data scrapper to collect written comments, comment titles, and rating score.</p>
            <p><strong>Dataset Size:</strong> 1,424 reviews</p>
            <p><strong>Ethical Considerations:</strong> We ensured that all data collected was publicly available and did not include any personally identifiable information.</p>

            <h3>Analysis Methods</h3>
            <p><strong>Tools:</strong> Python (pandas, VADER, Gensim)</p>
            <ul>
                <li><strong>Term Frequency Analysis:</strong> We used term frequency analysis to identify the most commonly words and phrases in the text. After preprocessing, we calculated how often each term appeared. This helped reveal the main themes and highlight which concepts the text emphasized most heavily.</li>
                <li><strong>Sentiment Analysis (VADER):</strong> We examined the overall sentiment of the text using VADER, focusing on the positive, negative, and neutral findings. We looked for patterns such as consistently positive or negative sections and changes in tone. This helped determine how the emotional content varied across the text.</li>
                <li><strong>Topic Modeling (Gensim LDA):</strong> We generated 5 topics using topic modeling. Each topic consisted of a group of related words, which revealed underlying themes from simple frequency counts. The discovered topics provided knowledge into the main subjects discussed in the text and how they are grouped together conceptually.</li>
            </ul>
        </section>

        <section id="analysis">
            <h2>Results & Analysis</h2>
            <!-- TODO: Add visualizations and code snippets -->

            <h3>Sentiment Analysis Results</h3>
            <p>Using VADER sentiment analysis, I examined [describe what you analyzed]...</p>

            <!-- Single visualization with caption -->
            <figure class="viz-container">
                <img src="images/sentiment-distribution.png"
                     alt="Bar chart showing distribution of positive, negative, and neutral sentiment">
                <figcaption>Figure 1: Distribution of sentiment scores across dataset</figcaption>
            </figure>

            <h3>Code Example</h3>
            <p>Here's how I implemented the sentiment analysis using <code>vaderSentiment</code>:</p>

            <div class="code-title">sentiment_analysis.py</div>
            <pre><code>from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd

analyzer = SentimentIntensityAnalyzer()

# Analyze sentiment for each text
df['compound'] = df['text'].apply(
    lambda x: analyzer.polarity_scores(x)['compound']
)

# Classify sentiment
df['sentiment'] = df['compound'].apply(
    lambda x: 'positive' if x > 0.05
    else ('negative' if x < -0.05 else 'neutral')
)</code></pre>

            <h3>Topic Modeling Results</h3>
            <p>Using Gensim's LDA implementation, I identified [number] major topics...</p>

            <!-- Multiple visualizations in a grid -->
            <div class="viz-grid">
                <figure class="viz-container">
                    <img src="images/figure_2.png"
                         alt="Visualization of topic clusters from LDA analysis">
                    <figcaption>Figure 2: Topic clusters from LDA analysis</figcaption>
                </figure>

                <figure class="viz-container">
                    <img src="images/.png"
                         alt="Word cloud showing most frequent terms">
                    <figcaption>Figure 3: Most common terms in the corpus</figcaption>
                </figure>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>
            <!-- TODO: Present your main discoveries -->

            <p>The computational analysis revealed three major insights:</p>

            <ol>
                <li><strong>Finding 1:</strong> Positive-sentiment reviews frequently paired empowering emotional language with trauma-related terms words such as ‚Äúface,‚Äù ‚Äústrong,‚Äù ‚Äúbrave,‚Äù or ‚Äúovercome.‚Äù This suggests that many reviewers used the film as a way to reflect on personal fears and frame their experiences in a more constructive or resilient way.</li>
                <li><strong>Finding 2:</strong> Negative-sentiment reviews showed a strong association with distress-oriented trauma language, including terms like ‚Äútriggered,‚Äù ‚Äúdisturbing,‚Äù ‚Äúpanic,‚Äù or ‚Äúunsettling.‚Äù This indicates that for some viewers, the film resurfaced unresolved fear or discomfort, reinforcing rather than easing emotional tension.</li>
                <li><strong>Finding 3:</strong> Topic modeling revealed that trauma-related language appeared across multiple themes not just in discussions of the story, but also in reviewers‚Äô reflections on their own lives. This pattern shows that horror operates as a shared cultural space where audiences connect their personal emotional histories to the narrative, using communal reviewing practices as a form of meaning-making and collective processing.</li>
            </ol>

            <h3>Detailed Results</h3>
            <p>Breaking down the sentiment distribution:</p>

            <table class="results-table">
                <thead>
                    <tr>
                        <th>Sentiment Category</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Avg. Compound Score</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight">
                        <td>Positive</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Neutral</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>0.XX</td>
                    </tr>
                    <tr>
                        <td>Negative</td>
                        <td>XXX</td>
                        <td>XX%</td>
                        <td>-0.XX</td>
                    </tr>
                </tbody>
            </table>

            <h3>What Surprised Me</h3>
            <p>We initially predicted that positive and negative reviews would show very clear, cleanly separated patterns of trauma language, but the data revealed more overlap than expected some positive reviews still used words associated with fear or distress, while some negative reviews included moments of reflection or empowerment. This challenged my understanding because it showed that emotional processing in horror is not binary. Viewers do not simply feel ‚Äúgood‚Äù or ‚Äúbad‚Äù‚Äîthey often experience trauma, fear, and empowerment simultaneously, and their language reflects that complexity.</p>
        </section>

        <section id="reflection">
            <h2>Critical Reflection</h2>
            <!-- TODO: Connect to course frameworks -->

            <p>This project demonstrates what happens when coding meets culture by revealing insights that neither computational analysis nor traditional close reading could discover alone. The computational methods allowed us to identify large-scale emotional and trauma-related patterns across hundreds of reviews, while close reading helped interpret the cultural meaning behind those patterns.
            What strengthened this project even more was how directly it connected to our course work. Throughout the semester, we explored how texts gain meaning not only through their content but through the communities that interact with them. The project showed that digital humanities tools do not replace interpretation; they expand it, enabling us to see emotional and cultural structures that align with theories we have studied about trauma, memory, audience reception, and participatory culture.
            </p>

            <h3>Integration of Methods</h3>
            <p><strong>What computational methods revealed:</strong> The computational analysis uncovered broad emotional patterns that only emerge at scale for example, the clustering of empowerment words (‚Äúface,‚Äù ‚Äústrong,‚Äù ‚Äúovercome‚Äù) with positive sentiment and distress-oriented trauma terms (‚Äútriggered,‚Äù ‚Äúdisturbing,‚Äù ‚Äúpanic‚Äù) with negative sentiment. It also revealed multi-topic connections between personal trauma language and plot discussion, showing that reviewers frequently blend interpretation of the film with autobiographical reflection. These trends would be nearly impossible to see through reading a handful of reviews.</p>
            <p><strong>What close reading added:</strong> Close reading helped explain why those patterns appeared. By examining specific reviews, it became clear that positive reviewers often described the film as cathartic, framing fear as something they could confront safely. Negative reviewers, however, used trauma language to express unresolved anxieties or discomfort with the film‚Äôs content. The interpretive work added nuance showing that horror reviews function as a space for identity, memory, and emotional processing, not just reaction. Close reading transformed the computational patterns from abstract data points into culturally meaningful insights about how people use horror to navigate trauma.</p>

            <div class="framework-callout">
                <h3>üìê Classification Logic</h3>
                <p>This project connects to <strong>Classification Logic</strong> by revealing how algorithmic categorization shapes our understanding of how reviewers use emotional language to process trauma. [Explain the connection...]</p>

                <p><em>Critical question:</em> What nuances are lost when we reduce complex cultural expressions to computational categories?</p>
            </div>

            <div class="framework-callout">
                <h3>ü§ñ AI Agency</h3>
                <p>The use of topic modeling and sentiment analysis demonstrates <strong>AI Agency</strong> concerns. While the algorithms appear to "discover" meaning, the interpretation and framing of results remains entirely human. [Explain further...]</p>
            </div>

            <h3>Limitations & Future Directions</h3>
            <p><strong>What I would do differently:</strong> [Reflect on your process]</p>
            <p><strong>Questions that remain:</strong> [What would you investigate with more time?]</p>
            <p><strong>Confidence in conclusions:</strong> [How certain are you about your findings? What caveats should readers consider?]</p>
        </section>
    </main>

    <footer>
        <p>üìä <strong>Project Materials:</strong>
            <a href="https://github.com/yourusername/project-name">View Google Colab Notebooks & Data on GitHub</a>
        </p>
        <p>&copy; 2025 Lucy Fulghum and Reagan Nowak | WRIT 20833: Introduction to Coding in the Humanities</p>
    </footer>
</body>
</html>
